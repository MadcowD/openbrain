%!TEX root = main.tex
\section{Experimentation}
\subsection{Experiment 1: Learning Q functions on components of the Actor network.}

\textbf{Motivation:}
Our first batch of experiments aimed to establish empirically that the Q-functions of each layer, denoted $Q_1...Q_L$ are similar to the Q-function of the overall network, denoted $Q_{\mu}$. To that end, we extended the actor-critic methodology used in Lillicrap et al (2016) as follows. In addition to training $Q^\mu$, to estimate the Q functions of the actor network $\mu$, sub-critic networks $Q^n$ were initialized for each individual layer. We then compare the $Q$ values estimated by the subcritics to the $Q$ estimation provided by the main critic, $Q^\mu$. 

To determine and compare the rate at which the subcritics and the main critic learn, the experiment was run in two phases. First, each of the subcritics and the main critic were trained using the standard DDPG algorithm on some actor $\mu$. In the second phase, a new actor $\mu'$ was initialized and its Q-function set to $Q_{\mu}$ as determined above in phase 1. The subcritics were trained, and the values of $Q_1\dots Q_{L}$ were plotted as training occurred.

\subsection{Experiment 2: Treating each neuron as its Actor-Critic network using linear approximators}

Now that we have established that the Critic networks for each of the individual neuron learns the same Q-function as does the entire agent, we now treat each neuron as its own actor. Each neuron changes the weights of its presynaptic neurons' connections, as parameters for its actor -- its voltage on the next timestep -- to optimize its learned approximated Q function. We use the linear approximation:

$$Q^{n}(v, a) \approx \theta_{n,v}^T v + \theta_{n,a}a = \theta_{n}^T (v, a)^T$$
$$\mu^{n}(v) = \sigma(K^n v)$$

Intuitively, it should be the case that by treating each neuron in the neural network as its own Q-learner, the policy gradient of an individual agent should be the same as for the policy gradient entire agent, if each neuron sees the same reward $r_t$ as the entire agent. In other words, the optimal way of updating the entire connection matrix $K$ should be the optimal way for updating the weights connected to each neuron w.r.t. each neuron as its own Q-learner, given the same rewards.

