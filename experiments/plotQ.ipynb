{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# EXPERIMENT 1:\n",
    "#\n",
    "# In this experiment we will train an actor according to a critic\n",
    "# and then simultaneously learn and plot the resulting\n",
    "################################################################\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from itertools import chain\n",
    "import gc\n",
    "import argparse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "gc.enable()\n",
    "\n",
    "from brain import DDPG\n",
    "from brain import SubCritics\n",
    "from brain.common.filter_env import makeFilteredEnv\n",
    "import brain.common.utils as utils\n",
    "from brain.common.utils import episode_stats, write_row\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(env, agent, num_tests):\n",
    "    \"\"\"\n",
    "    Tests the agent.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    for i in range(num_tests):\n",
    "        state = env.reset()\n",
    "        for j in range(env.spec.timestep_limit):\n",
    "            #env.render()\n",
    "            action = agent.action(state) # direct action for test\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "    avg_reward = total_reward/num_tests\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "def run_experiment(exp_name, ENV_NAME='MountainCarContinuous-v0', EPISODES=10000, TEST=10):\n",
    "    \"\"\"\n",
    "    Runs the experiment on the target en\n",
    "    \"\"\"\n",
    "    env = makeFilteredEnv(gym.make(ENV_NAME))\n",
    "\n",
    "    # Create the standard DDPG agent.\n",
    "    agent = DDPG(env)\n",
    "\n",
    "    sub_critics = SubCritics(agent, order=1) # Make linear (order 1) subcritics\n",
    "\n",
    "    # Set up tensorboard.\n",
    "    merged = tf.merge_all_summaries()\n",
    "    train_writer = tf.train.SummaryWriter('/tmp/tboard/{}'.format(exp_name),\n",
    "                                      agent.sess.graph)\n",
    "    # To see graph run tensorboard --logdir=/tmp/exp1/tboard\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    agent.sess.run(init_op)\n",
    "\n",
    "    criticToPlot = sub_critics.critics[0]\n",
    "    minPos = -1.2\n",
    "    maxPos = 0.6\n",
    "    minVelocity = -0.07\n",
    "    maxVelocity = 0.07\n",
    "    nx = 400\n",
    "    ny = 400\n",
    "    xRange = np.linspace(minPos, maxPos, nx)\n",
    "    yRange = np.linspace(minVelocity, maxVelocity, ny)\n",
    "    xv, yv = np.meshgrid(xRange, yRange)\n",
    "    zToPlot = []\n",
    "    xToPlot = xv[0]\n",
    "    yToPlot = [row[0] for row in yv]\n",
    "    t = 0\n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        activations = None\n",
    "        print(\"Episode: \", episode, end=\"\")\n",
    "        r_tot = 0\n",
    "\n",
    "        for step in range(env.spec.timestep_limit):\n",
    "            t+= 1\n",
    "            # Explore state space.\n",
    "            next_action, next_activations = agent.noise_action_activations(state)\n",
    "\n",
    "            # Deal with the environment\n",
    "            next_state,reward,done,_ = env.step(next_action)\n",
    "            r_tot += reward\n",
    "            # env.render()\n",
    "\n",
    "            # Train subcrticis and plot to tensorflow\n",
    "            if activations is not None and action is not None:\n",
    "                ops, feeds = sub_critics.get_perceive_run(activations, next_activations, reward, done)\n",
    "                ops += [\n",
    "                    agent.critic_network.q_value_output,\n",
    "                    agent.critic_network.target_q_value_output]\n",
    "                feeds.update({\n",
    "                    agent.critic_network.state_input: [state],\n",
    "                    agent.critic_network.action_input: [action],\n",
    "                    agent.critic_network.target_state_input: [next_state],\n",
    "                    agent.critic_network.target_action_input: [next_action]\n",
    "                    })\n",
    "                ops = [merged] + ops\n",
    "                result = agent.sess.run(ops, feeds)\n",
    "                # use ordered dict for stats_map\n",
    "                stats_result = agent.sess.run(episode_stats['variables'], feeds)\n",
    "                write_row(episode, step, stats_result)\n",
    "                train_writer.add_summary(result[0], t)\n",
    "\n",
    "            # Train DDPG\n",
    "            agent.perceive(state,next_action,reward,next_state,done)\n",
    "            if done:\n",
    "                break\n",
    "            # Move on to next frame.\n",
    "            state = next_state\n",
    "            activations = next_activations\n",
    "            action = next_action\n",
    "        print(\" \", r_tot)\n",
    "\n",
    "        # Testing:\n",
    "        if episode % 100 == 0 and episode > 100:\n",
    "            avg_reward = test(env, agent, TEST)\n",
    "            print(('episode: ',episode,'Evaluation Average Reward:',avg_reward))\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            x, y = xv[i,j], yv[i,j]\n",
    "            # tempAction = np.array([1])\n",
    "            # tempState = np.array([x,y])\n",
    "            # zToPlot.append(criticToPlot.q_value(tempState,tempAction))\n",
    "            ops = [agent.critic_network.q_value_output]\n",
    "            feeds = {}\n",
    "            state = np.array([x,y])[:,np.newaxis].T\n",
    "            action = np.array([1])[:,np.newaxis].T\n",
    "            feeds.update({\n",
    "                agent.critic_network.state_input: state,\n",
    "                agent.critic_network.action_input: action,\n",
    "                })\n",
    "            result = agent.sess.run(ops, feeds)\n",
    "            zToPlot.append(result[0].squeeze())\n",
    "    zToPlot = np.array(zToPlot).reshape((len(xToPlot), len(yToPlot)))\n",
    "    h = plt.contourf(xToPlot,yToPlot,zToPlot)\n",
    "    plt.title('Global q value')\n",
    "    plt.xlabel('position')\n",
    "    plt.ylabel('velocity')\n",
    "    plt.colorbar(h)\n",
    "    plt.show()\n",
    "    # criticToPlot = sub_critics.critics[0]\n",
    "    # tempAction = [[.5]]\n",
    "    # tempState = np.array([1,2])\n",
    "    # print(criticToPlot.q_value(tempAction,tempState))\n",
    "    # fixedAction = 0.5\n",
    "    # for velocity in range(-10,10):\n",
    "    #     for position in range(-1,1):\n",
    "    #         ops, feeds = sub_critics.get_perceive_run(activations, next_activations, reward, done)\n",
    "    #         ops += [\n",
    "    #             agent.critic_network.q_value_output,\n",
    "    #         feeds.update({\n",
    "    #             agent.critic_network.state_input: [state],\n",
    "    #             agent.critic_network.action_input: [fixedAction],\n",
    "    #             })\n",
    "    #         ops = [merged] + ops\n",
    "    #         result = agent.sess.run(ops, feeds)\n",
    "    #         # use ordered dict for stats_map\n",
    "    #         stats_result = agent.sess.run(episode_stats['variables'], feeds)\n",
    "    #         write_row(episode, step, stats_result)\n",
    "    #         train_writer.add_summary(result[0], t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO remove output dir\n",
    "output_dir = 'output/'\n",
    "exp_name='tboard'\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(output_dir)\n",
    "utils.set_output_dir(output_dir)\n",
    "# run_experiment(experiment_name, EPISODES=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_name='tboard'\n",
    "ENV_NAME='MountainCarContinuous-v0'\n",
    "EPISODES=70\n",
    "TEST=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-29 16:12:18,028] Making new env: MountainCarContinuous-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True action space: [-1.], [ 1.]\n",
      "True state space: [-1.2  -0.07], [ 0.6   0.07]\n",
      "Filtered action space: [-1.], [ 1.]\n",
      "Filtered state space: [-1. -1.], [ 1.  1.]\n",
      "DIMS 2 1\n",
      "Episode:  0  -13.396874741333322\n",
      "Episode:  1  -14.646075579940293\n",
      "Episode:  2  87.75209003655684\n",
      "Episode:  3  90.32768424231654\n",
      "Episode:  4"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Runs the experiment on the target en\n",
    "\"\"\"\n",
    "env = makeFilteredEnv(gym.make(ENV_NAME))\n",
    "\n",
    "# Create the standard DDPG agent.\n",
    "agent = DDPG(env)\n",
    "\n",
    "sub_critics = SubCritics(agent, order=1) # Make linear (order 1) subcritics\n",
    "\n",
    "# Set up tensorboard.\n",
    "merged = tf.merge_all_summaries()\n",
    "train_writer = tf.train.SummaryWriter('/tmp/tboard/{}'.format(exp_name),\n",
    "                                  agent.sess.graph)\n",
    "# To see graph run tensorboard --logdir=/tmp/exp1/tboard\n",
    "init_op = tf.initialize_all_variables()\n",
    "agent.sess.run(init_op)\n",
    "\n",
    "t = 0\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    activations = None\n",
    "    print(\"Episode: \", episode, end=\"\")\n",
    "    r_tot = 0\n",
    "\n",
    "    for step in range(env.spec.timestep_limit):\n",
    "        t+= 1\n",
    "        # Explore state space.\n",
    "        next_action, next_activations = agent.noise_action_activations(state)\n",
    "\n",
    "        # Deal with the environment\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        r_tot += reward\n",
    "        # env.render()\n",
    "\n",
    "        # Train subcrticis and plot to tensorflow\n",
    "        if activations is not None and action is not None:\n",
    "            ops, feeds = sub_critics.get_perceive_run(activations, next_activations, reward, done)\n",
    "            ops += [\n",
    "                agent.critic_network.q_value_output,\n",
    "                agent.critic_network.target_q_value_output]\n",
    "            feeds.update({\n",
    "                agent.critic_network.state_input: [state],\n",
    "                agent.critic_network.action_input: [action],\n",
    "                agent.critic_network.target_state_input: [next_state],\n",
    "                agent.critic_network.target_action_input: [next_action]\n",
    "                })\n",
    "            ops = [merged] + ops\n",
    "            result = agent.sess.run(ops, feeds)\n",
    "            # use ordered dict for stats_map\n",
    "            stats_result = agent.sess.run(episode_stats['variables'], feeds)\n",
    "            write_row(episode, step, stats_result)\n",
    "            train_writer.add_summary(result[0], t)\n",
    "\n",
    "        # Train DDPG\n",
    "        agent.perceive(state,next_action,reward,next_state,done)\n",
    "        if done:\n",
    "            break\n",
    "        # Move on to next frame.\n",
    "        state = next_state\n",
    "        activations = next_activations\n",
    "        action = next_action\n",
    "    print(\" \", r_tot)\n",
    "\n",
    "    # Testing:\n",
    "    if episode % 100 == 0 and episode > 100:\n",
    "        avg_reward = test(env, agent, TEST)\n",
    "        print(('episode: ',episode,'Evaluation Average Reward:',avg_reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criticToPlot = sub_critics.critics[0]\n",
    "minPos = -1.2\n",
    "maxPos = 0.6\n",
    "minVelocity = -0.07\n",
    "maxVelocity = 0.07\n",
    "nx = 400\n",
    "ny = 400\n",
    "xRange = np.linspace(minPos, maxPos, nx)\n",
    "yRange = np.linspace(minVelocity, maxVelocity, ny)\n",
    "xv, yv = np.meshgrid(xRange, yRange)\n",
    "zToPlot = []\n",
    "xToPlot = xv[0]\n",
    "yToPlot = [row[0] for row in yv]\n",
    "\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        x, y = xv[i,j], yv[i,j]\n",
    "        # tempAction = np.array([1])\n",
    "        # tempState = np.array([x,y])\n",
    "        # zToPlot.append(criticToPlot.q_value(tempState,tempAction))\n",
    "        ops = [agent.critic_network.q_value_output]\n",
    "        feeds = {}\n",
    "        state = np.array([x,y])[:,np.newaxis].T\n",
    "        action = np.array([1])[:,np.newaxis].T\n",
    "        feeds.update({\n",
    "            agent.critic_network.state_input: state,\n",
    "            agent.critic_network.action_input: action,\n",
    "            })\n",
    "        result = agent.sess.run(ops, feeds)\n",
    "        zToPlot.append(result[0].squeeze())\n",
    "zToPlot = np.array(zToPlot).reshape((len(xToPlot), len(yToPlot)))\n",
    "h = plt.contourf(xToPlot,yToPlot,zToPlot)\n",
    "plt.title('Global q value')\n",
    "plt.xlabel('position')\n",
    "plt.ylabel('velocity')\n",
    "plt.colorbar(h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criticToPlot = sub_critics.critics[0]\n",
    "minPos = -1.2\n",
    "maxPos = 0.6\n",
    "minVelocity = -0.07\n",
    "maxVelocity = 0.07\n",
    "nx = 400\n",
    "ny = 400\n",
    "xRange = np.linspace(minPos, maxPos, nx)\n",
    "yRange = np.linspace(minVelocity, maxVelocity, ny)\n",
    "xv, yv = np.meshgrid(xRange, yRange)\n",
    "zToPlot = []\n",
    "xToPlot = xv[0]\n",
    "yToPlot = [row[0] for row in yv]\n",
    "\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        x, y = xv[i,j], yv[i,j]\n",
    "        # tempAction = np.array([1])\n",
    "        # tempState = np.array([x,y])\n",
    "        # zToPlot.append(criticToPlot.q_value(tempState,tempAction))\n",
    "        ops = [agent.critic_network.q_value_output]\n",
    "        feeds = {}\n",
    "        state = np.array([x,y])[:,np.newaxis].T\n",
    "        action = np.array([-1])[:,np.newaxis].T\n",
    "        feeds.update({\n",
    "            agent.critic_network.state_input: state,\n",
    "            agent.critic_network.action_input: action,\n",
    "            })\n",
    "        result = agent.sess.run(ops, feeds)\n",
    "        zToPlot.append(result[0].squeeze())\n",
    "zToPlot = np.array(zToPlot).reshape((len(xToPlot), len(yToPlot)))\n",
    "h = plt.contourf(xToPlot,yToPlot,zToPlot)\n",
    "plt.title('Global q value')\n",
    "plt.xlabel('position')\n",
    "plt.ylabel('velocity')\n",
    "plt.colorbar(h)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
